{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import sys\n",
    "import powerlaw as pl\n",
    "sys.path.append('/Users/css/dev/thesis/selfish_mining_abm/selfish_mining_abm')\n",
    "import blockchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO LOOP OVER\n",
    "alphas = np.linspace(0, 0.5, 3)\n",
    "alpha = 0.3\n",
    "# 1 minute equals 60'000 milliseconds.\n",
    "gammas = np.linspace(100, 500, 1) / 60000\n",
    "gamma = 0.1\n",
    "# number of repetitions to average results over\n",
    "repetitions = 1\n",
    "\n",
    "# Hand/pick selfish miners: \"RANDOM\", \"BETWEENNESS\"\n",
    "# centrality_measures = [\"RANDOM\", \"BETWEENNESS\"]\n",
    "centrality_measure = \"RANDOM\"\n",
    "\n",
    "# available topologogies: \"UNIFORM\", \"ER\", \"BA\"\n",
    "# topologies = [\"UNIFORM\", \"ER\", \"BA\"]\n",
    "topology = \"UNIFORM\"\n",
    "\n",
    "# available hashing power distributions: \"UNIFORM\", \"POWERLAW\", \"EXPONENTIAL\"\n",
    "# hash_distributions = [\"UNIFORM\", \"POWERLAW\", \"EXPONENTIAL\"]\n",
    "hash_distribution = \"UNIFORM\"\n",
    "\n",
    "# SPECFIY TOPOLOGY\n",
    "desired_avg_degree = 5  # applies to ER and RAND topology.\n",
    "ba_m = 5  # relevant for BA topology; no. edges to attach from new node to existing nodes\n",
    "\n",
    "# SPECIFY HASHING POWER DISTRIBUTION\n",
    "pl_alpha = 1.88  # input parameter for powerlaw distribution\n",
    "exp_lambda = 1  # input parameter for exponential distribution\n",
    "# ADDITIONAL PARAMETERS\n",
    "simulation_time = 1000\n",
    "number_of_nodes = 100\n",
    "number_selfish_nodes = 1  # if there is more than 1 selfish miner, they act as \"cartel\"\n",
    "number_honest_nodes = number_of_nodes - number_selfish_nodes\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_topology(topology, number_of_nodes, desired_avg_degree, ba_m):\n",
    "    # generate network depending on topology parameter\n",
    "    if topology == \"UNIFORM\":\n",
    "        net_p2p = nx.random_degree_sequence_graph(\n",
    "            [desired_avg_degree for i in range(number_of_nodes)])\n",
    "\n",
    "    elif topology == \"ER\":\n",
    "        p = desired_avg_degree / number_of_nodes\n",
    "        net_p2p = nx.fast_gnp_random_graph(number_of_nodes, p)\n",
    "\n",
    "    elif topology == \"BA\":\n",
    "        net_p2p = nx.barabasi_albert_graph(number_of_nodes, ba_m)\n",
    "\n",
    "    # get largest connected component\n",
    "    lcc_set = max(nx.connected_components(net_p2p), key=len)\n",
    "    net_p2p = net_p2p.subgraph(lcc_set).copy()\n",
    "    # some nodes may have been removed because they were not port of the lcc.\n",
    "    # relabel nodes so that only nodes in lcc are labelled. (without it we run into problems where node labels are higher than the number of nodes -> loops run into indexing problems)\n",
    "    net_p2p = nx.convert_node_labels_to_integers(\n",
    "        net_p2p, first_label=0)\n",
    "\n",
    "    # some nodes may have been removed as they were not part of the lcc -> update num nodes\n",
    "    number_of_nodes = len(net_p2p)\n",
    "    # (brute-forcing number of selfish nodes to stay unchanged)\n",
    "    number_honest_nodes = number_of_nodes - number_selfish_nodes\n",
    "\n",
    "    return net_p2p, number_honest_nodes\n",
    "\n",
    "def set_up_hash_distr(net_p2p, centrality_measure, hash_distribution, number_selfish_nodes, number_honest_nodes, alpha):\n",
    "    # make sure that when there are no selfish nodes that alpha is never unequal 0. (in case you want to simulate only honest nodes)\n",
    "    assert not (number_selfish_nodes == 0 and alpha !=\n",
    "                0), \"Alpha unequal 0 with no selfish nodes\"\n",
    "\n",
    "    if hash_distribution == \"UNIFORM\":\n",
    "        hashing_power_selfish = np.random.random(number_selfish_nodes)\n",
    "        hashing_power_honest = np.random.random(number_honest_nodes)\n",
    "\n",
    "    elif hash_distribution == \"POWERLAW\":\n",
    "        power_distrib = pl.Power_Law(parameters=[pl_alpha], discrete=False)\n",
    "        hashing_power_selfish = power_distrib.generate_random(\n",
    "            number_selfish_nodes)\n",
    "        hashing_power_honest = power_distrib.generate_random(\n",
    "            number_honest_nodes)\n",
    "\n",
    "    elif hash_distribution == \"EXPONENTIAL\":\n",
    "        exp_distrib = pl.Exponential(parameters=[exp_lambda])\n",
    "        hashing_power_selfish = exp_distrib.generate_random(\n",
    "            number_selfish_nodes)\n",
    "        hashing_power_honest = exp_distrib.generate_random(\n",
    "            number_honest_nodes)\n",
    "\n",
    "    # normalize vector so that sum of selfish hashing power equals alpha & honest hashing power equals 1-alpha.\n",
    "    if number_selfish_nodes != 0:\n",
    "        hashing_power_selfish /= sum(hashing_power_selfish)\n",
    "        hashing_power_selfish *= alpha\n",
    "    hashing_power_honest /= sum(hashing_power_honest) / (1 - alpha)\n",
    "\n",
    "    # combine selfish and honest hashing power vectors together\n",
    "    hashing_power_unsorted = np.append(\n",
    "        hashing_power_selfish, hashing_power_honest)\n",
    "\n",
    "    if centrality_measure == \"RANDOM\":\n",
    "        # create an is_selfish vector that corresponds to the order of the hashing_power vector\n",
    "        is_selfish = np.append(np.ones(number_selfish_nodes),\n",
    "                                np.zeros(number_honest_nodes))\n",
    "\n",
    "        # finally, randomize is_selfish and hashing_power arrays in unison\n",
    "        randomize = np.arange(len(hashing_power_unsorted))\n",
    "        np.random.shuffle(randomize)\n",
    "        hashing_power = hashing_power_unsorted[randomize]\n",
    "        is_selfish = is_selfish[randomize]\n",
    "\n",
    "    elif centrality_measure == \"BETWEENNESS\":\n",
    "        # compute betweenness centrality and sort it\n",
    "        btwn = nx.betweenness_centrality(net_p2p)\n",
    "        btwn_sorted = {k: v for k, v in sorted(\n",
    "            btwn.items(), key=lambda item: item[1], reverse=True)}\n",
    "        # return node indeces sorted for betweenness centrality\n",
    "        btwn_sorted_indices = list(btwn_sorted.keys())\n",
    "\n",
    "        selfish_indices = list(btwn_sorted.keys())[:number_selfish_nodes]\n",
    "        honest_indices = list(btwn_sorted.keys())[\n",
    "            number_selfish_nodes:len(btwn)]\n",
    "\n",
    "        # set selifsh nodes according to betweenness centrality\n",
    "        is_selfish = np.zeros(number_honest_nodes+number_selfish_nodes)\n",
    "        for i in selfish_indices:\n",
    "            is_selfish[i] = 1\n",
    "\n",
    "        # sort hashing power vector so that selfish nodes are assigned correct hashing power\n",
    "        hashing_power = hashing_power_unsorted.copy()\n",
    "        for (index, value) in enumerate(btwn_sorted):\n",
    "            hashing_power[value] = hashing_power_unsorted[index]\n",
    "\n",
    "    return hashing_power, is_selfish\n",
    "\n",
    "def set_up_model(\n",
    "    centrality_measure, topology, hash_distribution, number_of_nodes, number_selfish_nodes, alpha, desired_avg_degree, ba_m\n",
    "):\n",
    "    net_p2p, number_honest_nodes = set_up_topology(\n",
    "        topology, number_of_nodes, desired_avg_degree, ba_m)\n",
    "\n",
    "    hashing_power, is_selfish = set_up_hash_distr(\n",
    "        net_p2p, centrality_measure, hash_distribution, number_selfish_nodes, number_honest_nodes, alpha)\n",
    "\n",
    "    return net_p2p, hashing_power, is_selfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(centrality_measure, topology, hash_distribution, number_of_nodes, number_selfish_nodes, alpha, desired_avg_degree, ba_m):\n",
    "    if alpha == 0:\n",
    "        net_p2p, hashing_power, is_selfish = set_up_model(\n",
    "            centrality_measure, topology, hash_distribution, number_of_nodes, number_selfish_nodes, 0, desired_avg_degree, ba_m\n",
    "        )\n",
    "    else:\n",
    "        net_p2p, hashing_power, is_selfish = set_up_model(\n",
    "            centrality_measure, topology, hash_distribution, number_of_nodes, number_selfish_nodes, alpha, desired_avg_degree, ba_m\n",
    "        )\n",
    "\n",
    "    model = blockchain.GillespieBlockchain(\n",
    "        net_p2p, is_selfish, hashing_power, gamma, verbose=verbose\n",
    "    )\n",
    "\n",
    "    while model.time < simulation_time:\n",
    "        model.next_event()\n",
    "    model.block_tree.tag_main_chain()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simulate(centrality_measure, topology, hash_distribution, number_of_nodes, number_selfish_nodes, alpha, desired_avg_degree, ba_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "### You can play around and access everything from above simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.block_tree.max_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of miner ID's for all mainchain blocks\n",
    "mc_miner_id_list = []\n",
    "\n",
    "for block in model.block_tree.tree.nodes():\n",
    "    if model.block_tree.attributes[block][\"main_chain\"]:\n",
    "        mc_miner_id_list.append(model.block_tree.attributes[block][\"miner\"])\n",
    "\n",
    "# remove genesis block\n",
    "mc_miner_id_list.pop(0)\n",
    "\n",
    "# Compute C_i value for honest miners\n",
    "C_i = 0\n",
    "# iterating over mc_miner_id_list without the last value so that I don't run into indexing issues.\n",
    "for (index, value) in enumerate(mc_miner_id_list[:-1]):\n",
    "    # first check: that miner mined two consecutive blocks | second check: that miner is honest\n",
    "    if mc_miner_id_list[index] == mc_miner_id_list[index+1] and model.is_selfish[value] == False:\n",
    "        C_i += 1\n",
    "\n",
    "# Compute S_i value\n",
    "# shuffle chain\n",
    "repititions = 100\n",
    "S_i_list = []\n",
    "for rep in range(repititions):\n",
    "    # shuffle chain\n",
    "    shuffled_mc_miner_id_list = mc_miner_id_list.copy()\n",
    "    random.shuffle(shuffled_mc_miner_id_list)\n",
    "    # compute average S_i value for each honest miner\n",
    "    C_i_rnd = 0\n",
    "\n",
    "    for (index, value) in enumerate(shuffled_mc_miner_id_list[:-1]):\n",
    "        # first check: that miner mined two consecutive blocks | second check: that miner is honest\n",
    "        if (shuffled_mc_miner_id_list[index] == shuffled_mc_miner_id_list[index+1]) and (model.is_selfish[value] == False):\n",
    "            C_i_rnd += 1\n",
    "    S_i_list.append(C_i_rnd)\n",
    "\n",
    "avg_S_i = np.mean(S_i_list)\n",
    "std_S_i = np.std(S_i_list)\n",
    "\n",
    "if std_S_i != 0:\n",
    "    msb_i = (C_i - avg_S_i) / std_S_i\n",
    "else:\n",
    "    msb_i = C_i - avg_S_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "0.26\n",
      "1.4729467430173613\n"
     ]
    }
   ],
   "source": [
    "print(C_i)\n",
    "print(S_i_list)\n",
    "print(avg_S_i)\n",
    "print(msb_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[70, 17, 14, 78, 79, 15, 15, 15, 33, 3, 6, 2, 57, 9, 95, 94, 71, 13, 4, 26, 52, 46, 11, 43, 47, 86, 4, 21, 15, 15, 15, 67, 95, 13, 77, 56, 15, 15, 75, 20, 52, 13, 30, 87, 15, 15, 15, 53, 97, 97, 22, 86, 14, 20, 15, 15, 15, 15, 15, 7, 6, 34, 15]\n",
      "[56, 6, 77, 15, 15, 53, 3, 22, 52, 6, 14, 15, 7, 20, 15, 33, 15, 21, 15, 70, 15, 15, 52, 95, 87, 9, 15, 15, 97, 15, 15, 67, 15, 75, 43, 20, 11, 15, 47, 14, 71, 30, 94, 95, 26, 86, 79, 34, 13, 17, 57, 15, 4, 15, 78, 4, 2, 13, 15, 46, 97, 13, 86]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(model.is_selfish)\n",
    "print(mc_miner_id_list)\n",
    "print(shuffled_mc_miner_id_list)\n",
    "print(S_i_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hashing power centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters needed\n",
    "centrality_measure = \"BETWEENNESS\"\n",
    "hash_distribution = \"UNIFORM\"\n",
    "number_honest_nodes = 9\n",
    "number_selfish_nodes = 1\n",
    "net_p2p = nx.erdos_renyi_graph(number_selfish_nodes+number_honest_nodes, 0.1)\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that when there are no selfish nodes that alpha is never unequal 0. (in case you want to simulate only honest nodes)\n",
    "assert not (number_selfish_nodes == 0 and alpha !=\n",
    "            0), \"Alpha unequal 0 with no selfish nodes\"\n",
    "\n",
    "if hash_distribution == \"UNIFORM\":\n",
    "    hashing_power_selfish = np.random.random(number_selfish_nodes)\n",
    "    hashing_power_honest = np.random.random(number_honest_nodes)\n",
    "\n",
    "# normalize vector so that sum of selfish hashing power equals alpha & honest hashing power equals 1-alpha.\n",
    "if number_selfish_nodes != 0:\n",
    "    hashing_power_selfish /= sum(hashing_power_selfish)\n",
    "    hashing_power_selfish *= alpha\n",
    "hashing_power_honest /= sum(hashing_power_honest) / (1 - alpha)\n",
    "\n",
    "# combine selfish and honest hashing power vectors together\n",
    "hashing_power_unsorted = np.append(hashing_power_selfish, hashing_power_honest)\n",
    "\n",
    "hashing_power_sorted = np.append(\n",
    "    sorted(hashing_power_selfish, reverse=True),\n",
    "    sorted(hashing_power_honest, reverse=True)\n",
    ")\n",
    "\n",
    "### \"BETWEENNESS\":\n",
    "# compute betweenness centrality and sort it\n",
    "btwn = nx.betweenness_centrality(net_p2p)\n",
    "btwn_sorted = {k: v for k, v in sorted(\n",
    "    btwn.items(), key=lambda item: item[1], reverse=True)}\n",
    "# return node indeces sorted for betweenness centrality\n",
    "btwn_sorted_indices = list(btwn_sorted.keys())\n",
    "\n",
    "selfish_indices = list(btwn_sorted.keys())[:number_selfish_nodes]\n",
    "# honest_indices = list(btwn_sorted.keys())[\n",
    "#     number_selfish_nodes:len(btwn)]\n",
    "\n",
    "# set selifsh nodes according to betweenness centrality\n",
    "is_selfish = np.zeros(number_honest_nodes+number_selfish_nodes)\n",
    "for i in selfish_indices:\n",
    "    is_selfish[i] = 1\n",
    "\n",
    "# sort hashing power vector so that selfish nodes are assigned correct hashing power\n",
    "hashing_power_btwn = hashing_power_unsorted.copy()\n",
    "for (index, value) in enumerate(btwn_sorted_indices):\n",
    "    hashing_power_btwn[value] = hashing_power_unsorted[index]\n",
    "\n",
    "### \"HASHINGPOWER\":\n",
    "# compute betweenness centrality and sort it\n",
    "# sort hashing power vector so that selfish nodes are assigned correct hashing power\n",
    "hashing_power_hash = hashing_power_unsorted.copy()\n",
    "for (index, value) in enumerate(btwn_sorted_indices):\n",
    "    hashing_power_hash[value] = hashing_power_sorted[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 0, 1, 2, 3, 4, 5, 6, 8, 9]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "print(list(btwn_sorted.keys()))\n",
    "print(list(btwn_sorted.keys())[:number_selfish_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chainsplit stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chainsplit_stats():\n",
    "    # get mainchain and orphan block IDs\n",
    "    all_blocks = []\n",
    "    mc_blocks = []\n",
    "    orphan_blocks = []\n",
    "\n",
    "    for block in model.block_tree.tree.nodes():\n",
    "        if model.block_tree.attributes[block][\"main_chain\"]:\n",
    "            mc_blocks.append(model.block_tree.attributes[block][\"id\"])\n",
    "        else:\n",
    "            orphan_blocks.append(model.block_tree.attributes[block][\"id\"])\n",
    "        all_blocks.append(model.block_tree[block][\"id\"])\n",
    "    # disregard genesis block\n",
    "    mc_blocks.pop(0)\n",
    "    all_blocks.pop(0)\n",
    "            \n",
    "    # COUNT NUMBER OF MAINCHAIN SPLITS\n",
    "    # count number of main chain splits (disregarding genesis block)\n",
    "    mainchain_split_count = 0\n",
    "    for orphan in orphan_blocks:\n",
    "        parent_block = list(model.block_tree.tree.predecessors(orphan))[0]\n",
    "        if model.block_tree.attributes[parent_block][\"main_chain\"] and not model.block_tree[parent_block][\"miner\"] == \"genesis\":\n",
    "            mainchain_split_count += 1\n",
    "\n",
    "\n",
    "    # COUT NUMBER OF ALL CHAIN SPLITS (ORPHAN & MAINCHAIN)\n",
    "    # count number of chain splits (disregarding genesis block)\n",
    "    chain_split_count = 0\n",
    "    for block in all_blocks:\n",
    "        num_children = len(list(model.block_tree.tree.successors(block)))\n",
    "        if num_children > 1:\n",
    "            chain_split_count += (num_children - 1)\n",
    "\n",
    "    # COMPUTE PROBABILITY OF MAIN CHAIN BLOCK HAVING NO SIBLINGS       \n",
    "    # count blocks on main chain that have more than 1 child (disregarding genesis block)\n",
    "    mainchain_no_sibling_count = 0\n",
    "    for block in mc_blocks:\n",
    "        if not len(list(model.block_tree.tree.successors(block))) > 1:\n",
    "            no_sibling_count += 1\n",
    "    return mainchain_split_count, chain_split_count, mainchain_no_sibling_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitf1f7cf62b47e4f109b07fbee7fbe52a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}